{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145baae8-32d0-46c3-a28c-e96fc8773d2a",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 53%; text-align: right;\">\n",
    "        <a >1</a>\n",
    "        <a href=\"02_introduction_mcp.ipynb\">2</a>\n",
    "        <a href=\"03_low_level_mcp.ipynb\">3</a>\n",
    "        <a href=\"04_langraph_agent.ipynb\">4</a>\n",
    "        <a href=\"05_challenge.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 46%; text-align: right;\"><a href=\"02_introduction_mcp.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177bf83c-6e87-4f15-b8c9-519f4c3b294d",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c649a-9a61-4b32-b19a-ac9ec6eb5e3d",
   "metadata": {},
   "source": [
    "By the end of this module, participants will be able to:\n",
    "- Understand what NVIDIA Inference Microservices (NIMs) are and their role in accelerated AI inference\n",
    "- Obtain and configure an NVIDIA API Key from the NVIDIA API Catalog\n",
    "- Send inference requests to NIM cloud or local endpoints using Python's requests library\n",
    "- Parse and interpret chat completion responses from the OpenAI-compatible NIM API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4495d79-cf1b-48e0-9853-e7637dacec11",
   "metadata": {},
   "source": [
    "## Using NVIDIA Inference Microservices (NIMs)\n",
    "\n",
    "NIMs are quickly accessible via easy-to-use open APIs available at [NVIDIA API Catalog](https://build.nvidia.com/explore/discover), a platform for accessing a wide range of microservices online. To start with NIMs, you need an `NVIDIA API Key` which requires registration. You can register by `clicking on the login button to enter your email address`, as shown in the screenshot below, and follow the rest process or attempt to generate the API Key via the [NVIDIA NGC](https://ngc.nvidia.com/signin) registration (*click on your account name -> setup -> Generate Personal Key*). After completing the process, please save your API Key somewhere you can access for future use. A sample API Key should start with `nvapi-` and 64 other characters, including underscore `_`.\n",
    "\n",
    "If you already have an account please follow this step to get your NVIDIA API KEY:\n",
    "\n",
    "- Login to your account from [here](https://build.nvidia.com/explore/discover).\n",
    "- Click on your model of choice.\n",
    "- Under Input select the Python tab, and click Get API Key and then click Generate Key.\n",
    "- Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/nim-catalog.png\" style=\"width: 900px; height: auto;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045d6c6-10ba-4818-abcf-bea62116266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "    os.environ[\"NGC_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b81b9-8b65-425e-90f5-655d8db63646",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This lab can be completed using either of the following methods:\n",
    "\n",
    "- **Cloud Endpoint** – Access NVIDIA NIMs via hosted on Cloud GPUs\n",
    "- **Local Endpoint** – Deploy NIMs locally on your own GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee3f19-76be-4dd1-bb16-267935905f68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cloud Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb87dd-78c0-4dcb-85ba-dd912241d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {os.environ['NVIDIA_API_KEY']}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"meta/llama-3.2-3b-instruct\",\n",
    "    \"messages\": [{\"role\": \"system\", \"content\": \"What is 2+2?\"}],\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 4096,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91c060-90d8-465e-b0d6-5ce871f3764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a330a-5490-42a1-81f9-804df910d961",
   "metadata": {},
   "source": [
    "## Local Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a0c4b-1f3c-4f89-8fb2-b928efb02baa",
   "metadata": {},
   "source": [
    "### Self-Hosted NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a913e91-5813-4439-845d-ec3ce00c8fdd",
   "metadata": {},
   "source": [
    "Please execute the cell below to ensure that your docker daemon is up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f220d8-4230-49a9-8f0c-5b897e32189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker ps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9158e6-9035-4e41-b6ef-e684a756348e",
   "metadata": {},
   "source": [
    "**Expected Output (if you have no running containers):**\n",
    "\n",
    "```python\n",
    "\n",
    "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b49cd-1b08-4ccb-8792-34bef18af815",
   "metadata": {},
   "source": [
    "### Login to NVCR (NVIDIA Container Registry)\n",
    "\n",
    "To access a NIM docker image, you must login via `docker login nvcr.io.` This process requires a default username as `--username $oauthtoken` and `--password-stdin` that accepts the value of `$NGC_API_KEY.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47160304-4ca6-482a-94c0-062ad0249bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo -e \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd0397-8000-4941-a5fc-b8027129f545",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "WARNING! Your password will be stored unencrypted in /home/yagupta/.docker/config.json.\n",
    "Configure a credential helper to remove this warning. See\n",
    "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
    "\n",
    "Login Succeeded\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1417fe-5b52-49cc-a80b-a1dcece37755",
   "metadata": {},
   "source": [
    "### Selection of NIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84b3d1-93fb-4ff2-bbe8-4e1cfbf01954",
   "metadata": {},
   "source": [
    "The [NIMs Catalog](https://build.nvidia.com/explore/reasoning) lists multiple state-of-the-art models in different domains. Look for the ones with the `RUN ANYWHERE` tag, as shown in the screenshot below. These NIM images are available to download and contain models and required optimized runtimes that help in getting started quickly.\n",
    "\n",
    "<img src=\"images/catalog3_0.png\" style=\"width: 900px; height: auto;\">\n",
    "\n",
    "Select the NIM model of your choice, click on the docker tab, and  copy the image name in the red box as shown in the screenshot below.  \n",
    "\n",
    "<img src=\"images/catalog3_1.png\" style=\"width: 900px; height: auto;\">\n",
    "<img src=\"images/catalog3_2.png\" style=\"width: 900px; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f0e5b-38df-4f75-83b0-018cdf0927fc",
   "metadata": {},
   "source": [
    "### Pull The Image \n",
    "\n",
    "The next step is to Pull the docker image. We demonstrate this step by pulling `llama-3.2-3b-instruct:latest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f9da7-d382-4028-b67c-97119ba73923",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker pull nvcr.io/nim/meta/llama-3.2-3b-instruct:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84683c59-cd18-4abb-88d3-ee913473d97c",
   "metadata": {},
   "source": [
    "**Likely output:** (When you have the image pulled already)\n",
    "```\n",
    "latest: Pulling from nim/meta/llama-3.2-3b-instruct\n",
    "Digest: sha256:b07b99986a134689c335958c3bb6a3d01ec7a46d0c308087b4d56384fc094708\n",
    "Status: Image is up to date for nvcr.io/nim/meta/llama-3.2-3b-instruct:latest\n",
    "nvcr.io/nim/meta/llama-3.2-3b-instruct:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f26ee-4ca0-4a1a-bb3a-ded35f50fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker image ls | grep llama-3.2-3b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787621d-3f6a-4067-abfd-89bddf8a35b5",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```python\n",
    "REPOSITORY                            TAG       IMAGE ID       CREATED        SIZE\n",
    "nvcr.io/nim/nvidia/llama-3.3-nemotron-super-49b-v1.5 latest 61eb40f73232 11 months ago 12.5GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f16597-8a03-4aad-b72e-f281ad12ec85",
   "metadata": {},
   "source": [
    "#### Setting up Cache for the Model Artifacts\n",
    "\n",
    "The NIMs download a number of files for ensuring the best profiles are selected to achieve max performance on hardware. Set up location for caching the model artifacts as `LOCAL_NIM_CACHE` and export the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41198cb5-6912-49cb-9fd0-c54d680efc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "os.environ['LOCAL_NIM_CACHE']=f\"{home}/.cache/nim\"\n",
    "!echo $LOCAL_NIM_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12297d-099e-4156-94a8-d438735c7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "# !chmod 777 \"$LOCAL_NIM_CACHE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d2f9c-4d77-468a-baf2-ab94afb0bc19",
   "metadata": {},
   "source": [
    "### Launch NIM LLM Microservice\n",
    "\n",
    "Launch the NIM LLM microservice by executing the docker run command in the cell bellow.\n",
    "\n",
    "```python\n",
    "docker run -it --rm -d --gpus all --name=llm_nim --shm-size=16GB  -e NGC_API_KEY  -v '$LOCAL_NIM_CACHE':/opt/nim/.cache  -u $(id -u) -p 8000:8000 nvcr.io/nim/meta/llama-3.2-3b-instruct:latest\n",
    "```\n",
    "\n",
    "This Docker command launches NIM LLM microservice using the following flags:\n",
    "\n",
    "- `-it`: Allocates a pseudo-TTY and keeps STDIN open for interactive processes\n",
    "- `--rm`: Automatically removes the container when it exits\n",
    "- `-d`: Runs the container in detached mode (in the background)\n",
    "- `--gpus all`: Allows the container to access all available GPUs\n",
    "- `--name=llm_nim`: Names the container \"llm_nim\"\n",
    "- `--shm-size=16GB`: Sets the size of /dev/shm to 16GB\n",
    "- `-e NGC_API_KEY`: Passes the NGC_API_KEY environment variable to the container\n",
    "- `-v $LOCAL_NIM_CACHE:/opt/nim/.cache`: Mounts the local NIM cache directory to /opt/nim/.cache in the container\n",
    "- `-u $(id -u)`: Runs the container with the current user's UID\n",
    "- `-p 8000:8000`: Maps port 8000 on the host to port 8000 in the container\n",
    "- `nvcr.io/nim/meta/llama-3.2-3b-instruct:latest`: Specifies the Docker image to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a721c-d72b-4603-b650-488abcae62bb",
   "metadata": {},
   "source": [
    "A system can have multiple running proceesses, so it is must to ensure we are not overtaking a port with any running application. The following code finds a unique free port and allots it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd2c78-44f0-4089-aab8-26c503f9dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import socket\n",
    "\n",
    "def find_available_port(start=11000, end=11999):\n",
    "    while True:\n",
    "        # Randomly select a port between start and end range\n",
    "        port = random.randint(start, end)\n",
    "        \n",
    "        # Try to create a socket and bind to the port\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            try:\n",
    "                sock.bind((\"localhost\", port))\n",
    "                # If binding is successful, the port is free\n",
    "                return port\n",
    "            except OSError:\n",
    "                # If binding fails, the port is in use, continue to the next iteration\n",
    "                continue\n",
    "\n",
    "# Find and print an available port\n",
    "os.environ['CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port: {os.environ['CONTAINER_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c14c7d-c3b2-4e5d-9a70-d73db94f5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it -d --rm \\\n",
    "--gpus \"device=${CUDA_VISIBLE_DEVICES}\" \\\n",
    "--name=llm_nim \\\n",
    "--shm-size=16GB  \\\n",
    "-e NGC_API_KEY \\\n",
    "-v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "-u $(id -u) \\\n",
    "-p $CONTAINER_PORT:8000 \\\n",
    "nvcr.io/nim/meta/llama-3.2-3b-instruct:latest\n",
    "\n",
    "# In order to ensure, the local NIM container is completely loaded and doesn't remain in pending stage, we instantiate a wait interval\n",
    "! sleep 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58386948-e1e8-4f30-824f-dbdbc3950021",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker logs --tail 45 llm_nim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec48cd9-2acc-415c-9112-ba20f9dabfb2",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "INFO 2026-01-09 10:25:32.249 on.py:62] Application startup complete.\n",
    "INFO 2026-01-09 10:25:32.251 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "INFO 2026-01-09 10:25:33.909 api_server.py:424] An example cURL request:\n",
    "curl -X 'POST' \\\n",
    "  'http://0.0.0.0:8000/v1/chat/completions' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"meta/llama-3.2-3b-instruct\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Hello! How are you?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"Hi! I am quite well, how can I help you today?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Can you write me a song?\"\n",
    "      }\n",
    "    ],\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 15,\n",
    "    \"stream\": true,\n",
    "    \"frequency_penalty\": 1.0,\n",
    "    \"stop\": [\"hello\"]\n",
    "  }'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92afbe4-e60b-4932-a899-f65c0afbf829",
   "metadata": {},
   "source": [
    "### Initiate A Quick Test\n",
    "You can quickly test that your NIM is up and running via two methods:\n",
    "- LangChain NVIDIA Endpoints\n",
    "- A simple OpenAI completion request\n",
    "\n",
    "**Parameter description:**\n",
    "- **base_url**: The ULR where the NIM docker image is deployed.\n",
    "- **model**: The name of the NIM model deployed. \n",
    "- **temperature**: To modulate the randomness of sampling. Reducing the temperature increases the chance of selecting words with high probabilities.\n",
    "- **top_p**: To control how deterministic the model is. If you are looking for exact and factual answers, keep this low. If you seek more diverse responses, increase to a higher value.\n",
    "- **max_tokens**: maximum number of output tokens to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdc031-07bf-4b2d-ab11-331f843d9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT']), model=\"meta/llama-3.2-3b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is 2+2?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccddb1e-3d0c-47a9-b01f-598ab1c866c3",
   "metadata": {},
   "source": [
    "In case of error outputs, wait for sometime and rerun the above cell. The error might be due to the NIM container not being up completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6eaab-875a-4d57-8508-73e7f490dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X 'POST' \\\n",
    "    \"http://0.0.0.0:${CONTAINER_PORT}/v1/completions\" \\\n",
    "    -H \"accept: application/json\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\"model\": \"meta/llama-3.2-3b-instruct\", \"prompt\": \"What is 2+2?\", \"max_tokens\": 64, \\\n",
    "    \"temperature\": 0.1, \"max_tokens\": 1000, \"top_p\": 1.0, \"stop\": [\".\"]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dc901-24bd-4654-9103-1616188a821a",
   "metadata": {},
   "source": [
    "### Links and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979bda9-1bc2-4247-b701-2b419ad44914",
   "metadata": {},
   "source": [
    "- [NVIDIA](https://docs.nvidia.com/nim/index.html)\n",
    "- [llama-3.2-3b-instruct](https://build.nvidia.com/meta/llama-3.2-3b-instruct) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85206769-bf7d-4f36-a192-4d7f2629eafa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d0de9-3804-4161-8ab5-2cb7b59a9621",
   "metadata": {},
   "source": [
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73df18-02a6-4877-99ef-6a52c060b171",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 53%; text-align: right;\">\n",
    "        <a >1</a>\n",
    "        <a href=\"02_introduction_mcp.ipynb\">2</a>\n",
    "        <a href=\"03_low_level_mcp.ipynb\">3</a>\n",
    "        <a href=\"04_langraph_agent.ipynb\">4</a>\n",
    "        <a href=\"05_challenge.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 46%; text-align: right;\"><a href=\"02_introduction_mcp.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
