{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1fb7c1-4775-4caa-b03d-0da8995bc266",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"03_low_level_mcp.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"01_inference_endpoint.ipynb\">1</a>\n",
    "        <a href=\"02_introduction_mcp.ipynb\">2</a>\n",
    "        <a href=\"03_low_level_mcp.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_challenge.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"05_challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74c2c6-4ddf-421a-9e5e-142471dbe672",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e7543-c7a8-41c1-8a0c-69420ab1ae35",
   "metadata": {},
   "source": [
    "By the end of this notebook, you will be able to:\n",
    "- Define LangGraph State schemas and build chatbot workflows using StateGraph, nodes, and edges\n",
    "- Connect NVIDIA NIM endpoints as the LLM backend using the `nvidia` model provider\n",
    "- Stream graph responses using `graph.stream()` for real-time output\n",
    "- Implement structured output with Pydantic models for parseable LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7191f-c627-433d-80e2-cc31f35e45ac",
   "metadata": {},
   "source": [
    "## Setup Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd92e1e-874f-4058-b875-17f27cb386d1",
   "metadata": {},
   "source": [
    "In the first notebook, we learned how to set up our generated `NVIDIA API KEY`. As a requirement for this notebook, you must set up the key as enviroment variable `NVIDIA_API_KEY` to pull the NIMs docker images of your choice. If you haven't gotten your key, please visit the NVIDIA NIMs API [homepage](https://build.nvidia.com/explore/discover) and generate your API Key. Please run the cell below, input your `NVIDIA API KEY` in the display textbox, and press the enter key on your keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450efe43-ec31-450d-ab22-9919fac8166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "    os.environ[\"NGC_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a93f79-3e8d-40c3-8484-b008f314de02",
   "metadata": {},
   "source": [
    "## Introduction to LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f99f86d-1773-4636-920a-2df900ff2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd3d252-7dec-49f6-8d4d-0ac9fa8d5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongenl/Documents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff788f-f593-46a6-9631-630412893e11",
   "metadata": {},
   "source": [
    "This creates a LangChain chat model connected to NVIDIA NIM. The `init_chat_model()` function handles all the configuration automatically—just specify the model ID and provider, and you're ready to start generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8940843-b9f3-4b62-9f13-5edbbfff9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'meta/llama-3.2-3b-instruct'\n",
    "llm = init_chat_model(model=MODEL_ID, model_provider=\"nvidia\",max_tokens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this out if you are using the local endpoint with right local port\n",
    "# LOCAL_CONTAINER_PORT = 11579\n",
    "# llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(CONTAINER_PORT), model=\"meta/llama-3.2-3b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d247020-6005-4649-bd22-8ff265d73717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf30eb9-4e33-49f7-8a21-32cfce39a8ce",
   "metadata": {},
   "source": [
    "In this section, we'll construct a simple agentic workflow using LangGraph's StateGraph. Here's what we'll do:\n",
    "\n",
    "1. Create a `StateGraph` with the state schema\n",
    "2. Add nodes using `add_node(name, function)`\n",
    "3. Add edges using `add_edge(source, target)`\n",
    "4. Compile the graph before execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d1bd6a-bc49-4ad3-ac00-4105f4cacafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state schema.\n",
    "    - messages: List of conversation messages with automatic append behavior\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac6440-891b-4f1b-a07f-534e662f854a",
   "metadata": {},
   "source": [
    "The State holds the conversation history using the `add_messages` reducer, which automatically appends new messages to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e00d3-9b92-457e-b1cc-8f4ecf83544f",
   "metadata": {},
   "source": [
    "Nodes are Python functions that receive state, perform actions, and return updated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab329c7-6216-4db3-b5ea-159d30ba7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    Chatbot node that invokes the LLM with conversation history.\n",
    "    Returns updated state with the assistant's response.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19afd452-c7f7-48d3-b93f-d8e7b5efc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add the chatbot node\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Connect START -> chatbot (entry point)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ea451",
   "metadata": {},
   "source": [
    "Use `graph.invoke()` to get synchronous complete responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594eaf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is chicago known for?', additional_kwargs={}, response_metadata={}, id='59951a82-5f27-41ee-827c-f3ae0f77f531'),\n",
       "  AIMessage(content='Chicago is known for its rich history, vibrant culture, and numerous attractions. Here are some of the top things Chicago is known for:\\n\\n1. **Architecture**:', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': 'Chicago is known for its rich history, vibrant culture, and numerous attractions. Here are some of the top things Chicago is known for:\\n\\n1. **Architecture**:', 'token_usage': {'prompt_tokens': 41, 'total_tokens': 73, 'completion_tokens': 32}, 'finish_reason': 'length', 'model_name': 'meta/llama-3.2-3b-instruct'}, id='lc_run--019beede-0638-73b0-8b1e-36e3ceef1eec-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 41, 'output_tokens': 32, 'total_tokens': 73}, role='assistant')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is chicago known for?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c540ae3-791c-4d14-b6d1-c0c980bf1c10",
   "metadata": {},
   "source": [
    "Use `graph.stream()` to get synchronous token-by-token responses, improving user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3964c6-2e71-43bd-94eb-f75ad1fb9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    \"\"\"Stream responses from the graph for real-time output.\"\"\"\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b372e3-047a-487d-9e24-4dd274442d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph_updates(\"what is portland known for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26957e95-85d7-42a7-860c-50e23c3357d7",
   "metadata": {},
   "source": [
    "## Structured Output with Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b176b-a7de-41fc-97d8-1e784d55d61c",
   "metadata": {},
   "source": [
    "Applications often need LLM responses in parseable formats (e.g., JSON) for downstream processing. NVIDIA NIM supports structured generation using guided JSON schemas. We use Pydantic's `BaseModel` to define the expected output structure. The `Literal` type restricts the output to specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f9580e-5dbe-40a2-87c7-1ca514701f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "class UserIntent(BaseModel):\n",
    "    \"\"\"The user's current intent in the conversation\"\"\"\n",
    "    intent: Literal[\"naruto\", \"bleach\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b1eec-77e4-454b-864a-c852de4fd99f",
   "metadata": {},
   "source": [
    "Reference: [NIM Structured Generation Docs](https://docs.nvidia.com/nim/large-language-models/latest/structured-generation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cc1e31-a74e-492a-a6ee-ffc4d59f7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_structured = init_chat_model(model=MODEL_ID, model_provider=\"nvidia\").with_structured_output(UserIntent, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f94a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(CONTAINER_PORT), model=MODEL_ID).with_structured_output(UserIntent, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d92d78-7708-4b1b-a527-db7eceb8fc8b",
   "metadata": {},
   "source": [
    "Use `.with_structured_output()` to enforce the Pydantic schema on LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb72b9d-6766-4937-8938-0d6ed0c70111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Classify user intent based on anime question\n",
    "res = llm_structured.invoke([\n",
    "    {'role':'system','content':'You are an anime encyclopedia. Classify if the user is asking a question on naruto or bleach.'},\n",
    "    {'role':'user','content':'who is sasuke?'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "844c6565-f815-48a9-a1d2-69c8519104ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent: intent='naruto'\n"
     ]
    }
   ],
   "source": [
    "print(f'intent: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a28fed",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ecaef",
   "metadata": {},
   "source": [
    "AI applications need memory to share context across multiple interactions.\n",
    "\n",
    "In LangGraph, you can add two types of memory:\n",
    "1) Short term memory (thread-level persistence) - this enables agents to track multi-turn conversations.\n",
    "2) Long term memory - use this to store user-specific or application-specific data across conversations.\n",
    "\n",
    "We will only utilise short term memory in this tutorial & challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "from langgraph.graph import StateGraph\n",
    "import json\n",
    "from langchain_core.messages import convert_to_openai_messages\n",
    "\n",
    "checkpointer = InMemorySaver()  \n",
    "\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)  \n",
    "\n",
    "res = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is cuda?\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086122d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(convert_to_openai_messages(res['messages']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13177f",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```json\n",
    "[{\"role\": \"user\", \"content\": \"what is cuda?\"}, {\"role\": \"assistant\", \"content\": \"CUDA (Parallel Computation Engine) is a parallel computing platform and programming model developed by NVIDIA. It allows developers to harness the power of multiple graphics processing units (\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "589bf0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what was my previous question?\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(convert_to_openai_messages(res['messages']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e328854",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```json\n",
    "[{\"role\": \"user\", \"content\": \"what is cuda?\"}, {\"role\": \"assistant\", \"content\": \"CUDA (Parallel Computation Engine) is a parallel computing platform and programming model developed by NVIDIA. It allows developers to harness the power of multiple graphics processing units (\"}, {\"role\": \"user\", \"content\": \"what was my previous question?\"}, {\"role\": \"assistant\", \"content\": \"Your previous question was: \\\\\"what is cuda?\\\\\"\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265621c5",
   "metadata": {},
   "source": [
    "By running `graph.invoke` with the same thread id, i.e. ` {\"configurable\": {\"thread_id\": \"1\"}}`, the graph keeps track of previous conversations and is able to utilise its history to continue the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44173d0",
   "metadata": {},
   "source": [
    "## Interrupts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22c566",
   "metadata": {},
   "source": [
    "Interrupts allow you to pause graph execution at specific points and wait for external input before continuing.  \n",
    "This enables human-in-the-loop patterns where you need external input to proceed.  \n",
    "When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "class FormState(TypedDict):\n",
    "    age: int | None\n",
    "\n",
    "def dummy_start_node(state: FormState):\n",
    "    print('start node')\n",
    "\n",
    "def get_age_node(state: FormState):\n",
    "    prompt = \"What is your age?\"\n",
    "\n",
    "    while True:\n",
    "        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n",
    "\n",
    "        if isinstance(answer, int) and answer > 0:\n",
    "            return {\"age\": answer}\n",
    "\n",
    "        prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n",
    "\n",
    "memory = InMemorySaver()\n",
    "\n",
    "builder = StateGraph(FormState)\n",
    "builder.add_node(\"dummy_start_node\",dummy_start_node)\n",
    "builder.add_node(\"collect_age\", get_age_node)\n",
    "builder.add_edge(START,\"dummy_start_node\")\n",
    "builder.add_edge(\"dummy_start_node\",\"collect_age\")\n",
    "builder.add_edge(\"collect_age\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"form-1\"}}\n",
    "first = graph.invoke({\"age\": None}, config=config)\n",
    "print(first[\"__interrupt__\"])  # -> [Interrupt(value='What is your age?', ...)]\n",
    "\n",
    "# Provide invalid data; the node re-prompts\n",
    "retry = graph.invoke(Command(resume=\"thirty\"), config=config)\n",
    "print(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)]\n",
    "\n",
    "# Provide valid data; loop exits and state updates\n",
    "final = graph.invoke(Command(resume=30), config=config)\n",
    "print(final[\"age\"])  # -> 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbbce50",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "start node.  \n",
    "[Interrupt(value='What is your age?', id='93589a0b323f03eeaa19f89000f5216c')].  \n",
    "[Interrupt(value=\"'thirty' is not a valid age. Please enter a positive number.\", id='93589a0b323f03eeaa19f89000f5216c')]. \n",
    "30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee954e5",
   "metadata": {},
   "source": [
    "The graph starts with `dummy_start_node` which prints 'start node'.\n",
    "\n",
    "In `graph.invoke({\"age\": None}, config=config)`, `{\"age\": None}` is passed as the state in the graph. This returns an interrupt of 'What is your age?\"\n",
    "\n",
    "In `graph.invoke(Command(resume=\"thirty\"), config=config)`, the state still retains the value of the initial invocation, i.e. `{\"age\": None}`; the value \"thirty\" in `Command(resume=\"thirty\")` is returned to the variable 'answer' in `get_age_node`.\n",
    "\n",
    "In `graph.invoke(Command(resume=30), config=config)`, the state does not change as above. The value '30' in `Command(resume=30)` is returned to the variable 'answer' in `get_age_node` and this is returned as the final value without an interrupt.\n",
    "\n",
    "<b>It is important to note that when running `graph.invoke(Command(resume=30), config=config)`, the node that raised an interrupt is rerun entirely; thus everything in the function `get_age_node` gets rerun from `prompt = \"What is your age?\"` each time.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce9c38",
   "metadata": {},
   "source": [
    "## Agent Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25effc",
   "metadata": {},
   "source": [
    "Agent skills are sets of instructions, scripts, and resources that agents can discover and load dynamically to perform better at specific tasks.  \n",
    "At its core, a skill is a folder containing a SKILL.md file. At a minimum, the skill.md file should contain 'name' and 'description' metadata fields. It can also contain instructions specifying the capabilities of the skill.\n",
    "\n",
    "Skills use progressive disclosure to manage context efficiently.\n",
    "* Discovery: At startup, agents load only the name and description of each available skill, just enough to know when it might be relevant.\n",
    "* Activation: When a task matches a skill’s description, the agent reads the instructions of the skill.\n",
    "* Execution: The agent follows the instructions found in the skill.\n",
    "\n",
    "Compared to the MCP protocol where the entire input schema has to be part of the agent's context right from the beginning, only the name and description of each skill is fed into the agent's context at the start and instructions are only loaded on a as needed basis.\n",
    "\n",
    "Skills can also optionally include scripts, references and assets. We'll skip these for the purpose of this tutorial & challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4b2b2",
   "metadata": {},
   "source": [
    "### Skills Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820ffaf",
   "metadata": {},
   "source": [
    "Create the `skills` folder. This will serve as the directory for all skills. We will only work with 1 skill for this tutorial - `sales-analytics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0201a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p skills\n",
    "!mkdir -p skills/sales-analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc0ed0",
   "metadata": {},
   "source": [
    "Following the [standard specification for skills](https://agentskills.io/specification), we\n",
    "\n",
    "1. Populate the name and description in the frontmatter. \n",
    "2. Fill up the instructions after the frontmatter. [optional]\n",
    "\n",
    "Following the [guidance](https://agentskills.io/specification#progressive-disclosure), the name and description fields makes up approximately 100 tokens while the instructions should be less than 5000 tokens.\n",
    "\n",
    "Standard SKILL.md template\n",
    "```\n",
    "---\n",
    "name: skill-name\n",
    "description: A description of what this skill does and when to use it.\n",
    "---\n",
    "<instructions in markdown here>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e9b1aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing skills/sales-analytics/SKILL.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile skills/sales-analytics/SKILL.md\n",
    "---\n",
    "name: sales-analytics\n",
    "description: Database schema and business logic for sales data analysis including customers, orders, and revenue.\n",
    "---\n",
    "# Sales Analytics Schema\n",
    "\n",
    "## Tables\n",
    "\n",
    "### customers\n",
    "- customer_id (PRIMARY KEY)\n",
    "- name\n",
    "- email\n",
    "- signup_date\n",
    "- status (active/inactive)\n",
    "- customer_tier (bronze/silver/gold/platinum)\n",
    "\n",
    "### orders\n",
    "- order_id (PRIMARY KEY)\n",
    "- customer_id (FOREIGN KEY -> customers)\n",
    "- order_date\n",
    "- status (pending/completed/cancelled/refunded)\n",
    "- total_amount\n",
    "- sales_region (north/south/east/west)\n",
    "\n",
    "### order_items\n",
    "- item_id (PRIMARY KEY)\n",
    "- order_id (FOREIGN KEY -> orders)\n",
    "- product_id\n",
    "- quantity\n",
    "- unit_price\n",
    "- discount_percent\n",
    "\n",
    "## Business Logic\n",
    "\n",
    "**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n",
    "\n",
    "**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\n",
    "\n",
    "**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\n",
    "\n",
    "**High-value orders**: Orders with total_amount > 1000\n",
    "\n",
    "## Example Query\n",
    "\n",
    "-- Get top 10 customers by revenue in the last quarter\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    c.customer_tier,\n",
    "    SUM(o.total_amount) as total_revenue\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE o.status = 'completed'\n",
    "  AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\n",
    "GROUP BY c.customer_id, c.name, c.customer_tier\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f29ae2",
   "metadata": {},
   "source": [
    "Import the necessary libraries.  `skills_ref` includes the helper functions that lists, validates and parses agent skills. This is based on the official [repo](https://github.com/agentskills/agentskills/tree/main) released by Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e01da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NotRequired\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelRequest, ModelResponse, AgentMiddleware, AgentState\n",
    "from langchain.messages import SystemMessage\n",
    "from typing import Callable\n",
    "from pathlib import Path\n",
    "from skills_ref.utils import list_skills\n",
    "from skills_ref.parser import read_instruction\n",
    "from skills_ref.models import SkillProperties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f89832",
   "metadata": {},
   "source": [
    "Define the state used by agents to store skills metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08ae4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillsState(AgentState):\n",
    "    \"\"\"State for the skills middleware.\"\"\"\n",
    "\n",
    "    skills_metadata: NotRequired[list[SkillProperties]]\n",
    "    \"\"\"List of loaded skill metadata (name, description).\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1dc583",
   "metadata": {},
   "source": [
    "Create the skill loading tool that loads the instructions from a skill's SKILL.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79bd01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skill loading tool\n",
    "@tool\n",
    "def load_skill(skills_dir: Path,skill_name: str) -> str:\n",
    "    \"\"\"Load the full instructions of a skill into the agent's context.\n",
    "\n",
    "    Use this when you need detailed information about how to handle a specific\n",
    "    type of request. This will provide you with comprehensive instructions,\n",
    "    policies, and guidelines for the skill area.\n",
    "\n",
    "    Args:\n",
    "        skill_name: The name of the skill to load (e.g., \"qna agent\")\n",
    "    \"\"\"\n",
    "    content = read_instruction(skills_dir / skill_name)\n",
    "    if content:\n",
    "        return f\"Loaded skill: {skill_name}\\n\\ncontent\"\n",
    "    else:\n",
    "        skills = list_skills(skills_dir)\n",
    "        available = \", \".join(skills.name for s in skills)\n",
    "        return f\"Skill '{skill_name}' not found. Available skills: {available}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12a052",
   "metadata": {},
   "source": [
    "Create custom middleware that injects skill descriptions into the system prompt.  \n",
    "The full list of properties and functions that can be implemented in the Agent Middleware interface can be found [here]((https://github.com/langchain-ai/langchain/blob/c930062f69bbf72d0147db2e2db1940777966ffe/libs/langchain_v1/langchain/agents/middleware/types.py#L343-L756)).  \n",
    "We are only interested in `state_schema`,`tools`, `before_agent` and `wrap_model_call` for the purpose of this tutorial & challenge.  \n",
    "We specify `tools = [load_skill]` to allow the agent to utilize the `load_skill` tool to read the instructions of the skills.  \n",
    "In `before_agent`, we retrieve the metadata(name, description) of the skills dynamically from the system directory and update the agent's `state_schema`. Refer to [utils.py](./skills_ref/utils.py) for more details.  \n",
    "In `wrap_model_call`, we retrieve the skills metadata from the agent's state, build the skills addendum and append it to the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skill middleware\n",
    "class SkillMiddleware(AgentMiddleware):\n",
    "    \"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\n",
    "\n",
    "    state_schema = SkillsState\n",
    "\n",
    "    # Register the load_skill tool as a class variable\n",
    "    tools = [load_skill]\n",
    "\n",
    "    def __init__(self,skills_dir):\n",
    "        self.skills_dir = skills_dir\n",
    "    \n",
    "    def before_agent(self, state:SkillsState, runtime):\n",
    "        skills = list_skills(self.skills_dir)\n",
    "        return SkillsState(skills_metadata=skills)\n",
    "\n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "        \"\"\"Sync: Inject skill descriptions into system prompt.\"\"\"\n",
    "\n",
    "        skills = request.state.get(\"skills_metadata\", [])\n",
    "        skills_list = []\n",
    "        for skill in skills:\n",
    "            skills_list.append(\n",
    "                f\"- **{self.skills_dir / skill.name}**: {skill.description}\"\n",
    "            )\n",
    "        self.skills_prompt = \"\\n\".join(skills_list)\n",
    "\n",
    "        # Build the skills addendum\n",
    "        skills_addendum = (\n",
    "            f\"\\n\\n## Available Skills\\n\\n{self.skills_prompt}\\n\\n\"\n",
    "            \"Use the load_skill tool when you need detailed information \"\n",
    "            \"about handling a specific type of request.\"\n",
    "        )\n",
    "\n",
    "        # Append to system message content blocks\n",
    "        new_content = list(request.system_message.content_blocks) + [\n",
    "            {\"type\": \"text\", \"text\": skills_addendum}\n",
    "        ]\n",
    "        new_system_message = SystemMessage(content=new_content)\n",
    "        modified_request = request.override(system_message=new_system_message)\n",
    "        return handler(modified_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3622b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "# model_id = \"deepseek-ai/deepseek-v3.2\"\n",
    "model_id = 'moonshotai/kimi-k2-thinking'\n",
    "nvidia_model = init_chat_model(model=model_id,base_url=\"https://integrate.api.nvidia.com/v1\",model_provider=\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd4a8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class SQLOutput(BaseModel):\n",
    "    sql: str = Field(description=\"runnable SQL query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f64b79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_agent(skills_dir,debug=False):\n",
    "    # Create the agent with skill support\n",
    "    agent = create_agent(\n",
    "        nvidia_model,\n",
    "        system_prompt=(\n",
    "            \"You are a SQL query assistant that generates runnable SQL query for a music database.\"\n",
    "        ),\n",
    "        middleware=[SkillMiddleware(skills_dir)],\n",
    "        # checkpointer=InMemorySaver(),\n",
    "        response_format=SQLOutput,\n",
    "        debug=debug\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10cb0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_dir = Path.cwd().resolve() / 'skills'\n",
    "agent = create_sql_agent(skills_dir,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50abf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(  \n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Write a SQL query to find all customers \"\n",
    "                    \"who made orders over $1000 in the last month\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the conversation\n",
    "for message in result[\"messages\"]:\n",
    "    if hasattr(message, 'pretty_print'):\n",
    "        message.pretty_print()\n",
    "    else:\n",
    "        print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e604778",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "================================ Human Message =================================\n",
    "\n",
    "Write a SQL query to find all customers who made orders over $1000 in the last month\n",
    "================================== Ai Message ==================================\n",
    "Tool Calls:\n",
    "  load_skill (functions.load_skill:0)\n",
    " Call ID: functions.load_skill:0\n",
    "  Args:\n",
    "    skills_dir: <root_dir>/agentic-ai-bootcamp/tutorial/jupyter_notebook/skills\n",
    "    skill_name: sales-analytics\n",
    "================================= Tool Message =================================\n",
    "Name: load_skill\n",
    "\n",
    "Loaded skill: sales-analytics\n",
    "\n",
    "content\n",
    "================================== Ai Message ==================================\n",
    "Tool Calls:\n",
    "  SQLOutput (functions.SQLOutput:1)\n",
    " Call ID: functions.SQLOutput:1\n",
    "  Args:\n",
    "    sql: SELECT DISTINCT c.customer_id, c.name, c.email, c.phone, o.order_date, o.total_amount, o.order_id\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE o.order_date >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month')\n",
    "  AND o.order_date < DATE_TRUNC('month', CURRENT_DATE)\n",
    "  AND o.total_amount > 1000\n",
    "ORDER BY o.total_amount DESC, c.customer_id;\n",
    "================================= Tool Message =================================\n",
    "Name: SQLOutput\n",
    "\n",
    "Returning structured response: sql=\"SELECT DISTINCT c.customer_id, c.name, c.email, c.phone, o.order_date, o.total_amount, o.order_id\\nFROM customers c\\nJOIN orders o ON c.customer_id = o.customer_id\\nWHERE o.order_date >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month')\\n  AND o.order_date < DATE_TRUNC('month', CURRENT_DATE)\\n  AND o.total_amount > 1000\\nORDER BY o.total_amount DESC, c.customer_id;\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcee021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['structured_response'].sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24df12d",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "SELECT DISTINCT c.customer_id, c.name, c.email, c.phone, o.order_date, o.total_amount, o.order_id\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "WHERE o.order_date >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month')\n",
    "  AND o.order_date < DATE_TRUNC('month', CURRENT_DATE)\n",
    "  AND o.total_amount > 1000\n",
    "ORDER BY o.total_amount DESC, c.customer_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55b41d-49af-4c4b-80d2-a64d6ad496d5",
   "metadata": {},
   "source": [
    "## Links and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2635a1f-d68e-4126-b2bc-786eee467ade",
   "metadata": {},
   "source": [
    "- [LangGraph repo](https://github.com/langchain-ai/langgraph)\n",
    "- [LangGraph short term memory](https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory)\n",
    "- [LangGraph Interrupts](https://docs.langchain.com/oss/python/langgraph/interrupts)\n",
    "- [LangChain Agent Skills](https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant)\n",
    "- [Agent skills open protocol](https://agentskills.io/home)\n",
    "- [LangChain NVIDIA](https://github.com/langchain-ai/langchain-nvidia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3f795-24aa-40f5-bbd0-f063d2dbfe65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114b816-92e4-4c71-a16e-b5c3c22d37aa",
   "metadata": {},
   "source": [
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6577e3c-5240-477f-90e9-c910483d1a10",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"03_low_level_mcp.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"01_inference_endpoint.ipynb\">1</a>\n",
    "        <a href=\"02_introduction_mcp.ipynb\">2</a>\n",
    "        <a href=\"03_low_level_mcp.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_agent_skills.ipynb\">5</a>\n",
    "        <a href=\"06_challenge.ipynb\">6</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"05_challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006de16f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Documents (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
