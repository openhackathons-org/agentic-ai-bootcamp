{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1fb7c1-4775-4caa-b03d-0da8995bc266",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"03_low_level_mcp.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"01_inference_endpoint.ipynb\">1</a>\n",
    "        <a href=\"02_introduction_mcp.ipynb\">2</a>\n",
    "        <a href=\"03_low_level_mcp.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_challenge.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"05_challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74c2c6-4ddf-421a-9e5e-142471dbe672",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e7543-c7a8-41c1-8a0c-69420ab1ae35",
   "metadata": {},
   "source": [
    "By the end of this notebook, you will be able to:\n",
    "- Define LangGraph State schemas and build chatbot workflows using StateGraph, nodes, and edges\n",
    "- Connect NVIDIA NIM endpoints as the LLM backend using the `nvidia` model provider\n",
    "- Stream graph responses using `graph.stream()` for real-time output\n",
    "- Implement structured output with Pydantic models for parseable LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7191f-c627-433d-80e2-cc31f35e45ac",
   "metadata": {},
   "source": [
    "## Setup Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd92e1e-874f-4058-b875-17f27cb386d1",
   "metadata": {},
   "source": [
    "In the first notebook, we learned how to set up our generated `NVIDIA API KEY`. As a requirement for this notebook, you must set up the key as enviroment variable `NVIDIA_API_KEY` to pull the NIMs docker images of your choice. If you haven't gotten your key, please visit the NVIDIA NIMs API [homepage](https://build.nvidia.com/explore/discover) and generate your API Key. Please run the cell below, input your `NVIDIA API KEY` in the display textbox, and press the enter key on your keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450efe43-ec31-450d-ab22-9919fac8166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "    os.environ[\"NGC_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a93f79-3e8d-40c3-8484-b008f314de02",
   "metadata": {},
   "source": [
    "## Introduction to LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99f86d-1773-4636-920a-2df900ff2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3d252-7dec-49f6-8d4d-0ac9fa8d5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff788f-f593-46a6-9631-630412893e11",
   "metadata": {},
   "source": [
    "This creates a LangChain chat model connected to NVIDIA NIM. The `init_chat_model()` function handles all the configuration automatically—just specify the model ID and provider, and you're ready to start generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8940843-b9f3-4b62-9f13-5edbbfff9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta/llama-3.2-3b-instruct'\n",
    "llm = init_chat_model(model=model_id, model_provider=\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d247020-6005-4649-bd22-8ff265d73717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf30eb9-4e33-49f7-8a21-32cfce39a8ce",
   "metadata": {},
   "source": [
    "In this section, we'll construct a simple agentic workflow using LangGraph's StateGraph. Here's what we'll do:\n",
    "\n",
    "1. Create a `StateGraph` with the state schema\n",
    "2. Add nodes using `add_node(name, function)`\n",
    "3. Add edges using `add_edge(source, target)`\n",
    "4. Compile the graph before execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d1bd6a-bc49-4ad3-ac00-4105f4cacafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state schema.\n",
    "    - messages: List of conversation messages with automatic append behavior\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac6440-891b-4f1b-a07f-534e662f854a",
   "metadata": {},
   "source": [
    "The State holds the conversation history using the `add_messages` reducer, which automatically appends new messages to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e00d3-9b92-457e-b1cc-8f4ecf83544f",
   "metadata": {},
   "source": [
    "Nodes are Python functions that receive state, perform actions, and return updated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab329c7-6216-4db3-b5ea-159d30ba7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    Chatbot node that invokes the LLM with conversation history.\n",
    "    Returns updated state with the assistant's response.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afd452-c7f7-48d3-b93f-d8e7b5efc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add the chatbot node\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Connect START -> chatbot (entry point)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c540ae3-791c-4d14-b6d1-c0c980bf1c10",
   "metadata": {},
   "source": [
    "Use `graph.stream()` to get real-time token-by-token responses, improving user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3964c6-2e71-43bd-94eb-f75ad1fb9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    \"\"\"Stream responses from the graph for real-time output.\"\"\"\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b372e3-047a-487d-9e24-4dd274442d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph_updates(\"what is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26957e95-85d7-42a7-860c-50e23c3357d7",
   "metadata": {},
   "source": [
    "## Structured Output with Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b176b-a7de-41fc-97d8-1e784d55d61c",
   "metadata": {},
   "source": [
    "Applications often need LLM responses in parseable formats (e.g., JSON) for downstream processing. NVIDIA NIM supports structured generation using guided JSON schemas. We use Pydantic's `BaseModel` to define the expected output structure. The `Literal` type restricts the output to specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9580e-5dbe-40a2-87c7-1ca514701f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "class UserIntent(BaseModel):\n",
    "    \"\"\"The user's current intent in the conversation\"\"\"\n",
    "    intent: Literal[\"naruto\", \"bleach\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b1eec-77e4-454b-864a-c852de4fd99f",
   "metadata": {},
   "source": [
    "Reference: [NIM Structured Generation Docs](https://docs.nvidia.com/nim/large-language-models/latest/structured-generation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc1e31-a74e-492a-a6ee-ffc4d59f7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_structured = init_chat_model(model=model_id, model_provider=\"nvidia\").with_structured_output(\n",
    "    UserIntent, strict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d92d78-7708-4b1b-a527-db7eceb8fc8b",
   "metadata": {},
   "source": [
    "Use `.with_structured_output()` to enforce the Pydantic schema on LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb72b9d-6766-4937-8938-0d6ed0c70111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Classify user intent based on anime question\n",
    "res = llm_structured.invoke([\n",
    "    {'role':'system','content':'You are an anime encyclopedia. Classify if the user is asking a question on naruto or bleach.'},\n",
    "    {'role':'user','content':'who is sasuke?'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c6565-f815-48a9-a1d2-69c8519104ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'intent: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55b41d-49af-4c4b-80d2-a64d6ad496d5",
   "metadata": {},
   "source": [
    "## Links and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2635a1f-d68e-4126-b2bc-786eee467ade",
   "metadata": {},
   "source": [
    "- [LangGraph](https://github.com/langchain-ai/langgraph)\n",
    "- [LangChain NVIDIA](https://github.com/langchain-ai/langchain-nvidia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3f795-24aa-40f5-bbd0-f063d2dbfe65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114b816-92e4-4c71-a16e-b5c3c22d37aa",
   "metadata": {},
   "source": [
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6577e3c-5240-477f-90e9-c910483d1a10",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"03_low_level_mcp.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 34%; text-align: center;\">\n",
    "        <a href=\"01_inference_endpoint.ipynb\">1</a>\n",
    "        <a href=\"02_introduction_mcp.ipynb\">2</a>\n",
    "        <a href=\"03_low_level_mcp.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_challenge.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"05_challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
